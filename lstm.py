# -*- coding: utf-8 -*-
"""lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11jIYxAT_70Lp16WKahGaZugQwX18S16D
"""

!pip install tensorflow numpy pandas matplotlib scikit-learn

import os

# create output folders
os.makedirs("outputs/logs", exist_ok=True)
os.makedirs("outputs/models", exist_ok=True)
os.makedirs("outputs/predictions", exist_ok=True)
os.makedirs("outputs/plots", exist_ok=True)

print("Project output folders created successfully.")

"""generated data"""

import numpy as np
import pandas as pd

def generate_dataset(n_samples=5000, seq_len=50):
    X = []
    y = []
    for _ in range(n_samples):
        base = np.sin(np.linspace(0, 3*np.pi, seq_len)) + np.random.normal(0, 0.1, seq_len)
        X.append(base)
        y.append(base[-1] + np.random.normal(0, 0.05))
    return np.array(X), np.array(y)

X, y = generate_dataset()

df = pd.DataFrame({"sample": list(range(len(y))), "target": y})
df.to_csv("outputs/predictions/dataset_preview.csv", index=False)

print("Dataset generated — Shape:", X.shape, y.shape)

np.save('outputs/X_data.npy', X)
np.save('outputs/y_data.npy', y)

print("X and y arrays saved to outputs/X_data.npy and outputs/y_data.npy")

import numpy as np
import pandas as pd

def generate_dataset(n_samples=5000, seq_len=50):
    X = []
    y = []
    for _ in range(n_samples):
        base = np.sin(np.linspace(0, 3*np.pi, seq_len)) + np.random.normal(0, 0.1, seq_len)
        X.append(base)
        y.append(base[-1] + np.random.normal(0, 0.05))
    return np.array(X), np.array(y)

X, y = generate_dataset()

df = pd.DataFrame({"sample": list(range(len(y))), "target": y})
df.to_csv("outputs/predictions/dataset_preview.csv", index=False)

print("Dataset generated — Shape:", X.shape, y.shape)



df.to_csv("outputs/predictions/dataset_preview.csv", index=False)
print("DataFrame saved to outputs/predictions/dataset_preview.csv")

df.head()

display(df.describe())

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Train:", X_train.shape, "Validation:", X_val.shape)

import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense

class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
        self.score_dense = Dense(1) # Instantiate Dense layer once in __init__

    def call(self, inputs):
        score = tf.nn.softmax(self.score_dense(inputs), axis=1) # Use the instantiated layer
        context = tf.reduce_sum(score * inputs, axis=1)
        return context, score

class ExpandDimsLayer(Layer):
    def __init__(self, axis=-1, **kwargs):
        super(ExpandDimsLayer, self).__init__(**kwargs)
        self.axis = axis

    def call(self, inputs):
        return tf.expand_dims(inputs, axis=self.axis)

    def get_config(self):
        config = super(ExpandDimsLayer, self).get_config()
        config.update({"axis": self.axis})
        return config

from tensorflow.keras import Input, Model
from tensorflow.keras.layers import LSTM, Dense

seq_len = X_train.shape[1]

inp = Input(shape=(seq_len,))
x = ExpandDimsLayer(axis=-1)(inp)
x = LSTM(64, return_sequences=True)(x)
context, attn_weights = AttentionLayer()(x)
out = Dense(1)(context)

model = Model(inputs=inp, outputs=[out, attn_weights])
model.compile(optimizer="adam", loss="mse")

model.summary()

import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

class AttentionLayer(Layer):
    def __init__(self):
        super(AttentionLayer, self).__init__()

    def call(self, inputs):
        score = tf.nn.softmax(Dense(1)(inputs), axis=1)
        context = tf.reduce_sum(score * inputs, axis=1)
        return context, score

class ExpandDimsLayer(Layer):
    def __init__(self, axis=-1, **kwargs):
        super(ExpandDimsLayer, self).__init__(**kwargs)
        self.axis = axis

    def call(self, inputs):
        return tf.expand_dims(inputs, axis=self.axis)

    def get_config(self):
        config = super(ExpandDimsLayer, self).get_config()
        config.update({"axis": self.axis})
        return config

from tensorflow.keras import Input, Model
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import numpy as np # Import numpy for dummy targets

seq_len = X_train.shape[1]

inp = Input(shape=(seq_len,))
x = ExpandDimsLayer(axis=-1)(inp)
x = LSTM(64, return_sequences=True)(x)
context, attn_weights = AttentionLayer()(x)
out = Dense(1)(context)

# Name the outputs for clarity and to easily specify losses
model = Model(inputs=inp, outputs=[out, attn_weights])

# Compile with a list of losses corresponding to each output
# 'mse' for the main prediction and None for attention weights (if not a target)
model.compile(optimizer="adam", loss=["mse", None])

model.summary()


model_checkpoint_callback = ModelCheckpoint(
    filepath='outputs/models/best_model.h5',
    save_weights_only=False,
    monitor='val_loss',
    mode='min',
    save_best_only=True,
    verbose=1
)

early_stopping_callback = EarlyStopping(
    monitor='val_loss',
    patience=10,
    mode='min',
    restore_best_weights=True,
    verbose=1
)

# Create dummy targets for attention weights
dummy_attn_train = np.zeros((X_train.shape[0], seq_len, 1))
dummy_attn_val = np.zeros((X_val.shape[0], seq_len, 1))

# Pass y_train and y_val as lists, with dummy targets for the second output (attn_weights)
history = model.fit(
    X_train,
    [y_train, dummy_attn_train],
    epochs=50,
    batch_size=32,
    validation_data=(X_val, [y_val, dummy_attn_val]),
    callbacks=[model_checkpoint_callback, early_stopping_callback]
)

print("Model training complete.")

history = model.fit(
    X_train,
    [y_train, np.zeros((len(y_train), seq_len, 1))],
    epochs=10,
    batch_size=32,
    validation_data=(X_val, [y_val, np.zeros((len(y_val), seq_len, 1))])
)

# save training logs
import json
with open("outputs/logs/training_logs.json", "w") as f:
    json.dump(history.history, f, indent=4)

print("Training complete — logs saved.")

model.save("outputs/models/final_model.h5")
model.save("outputs/models/final_model.keras")

print("Model saved successfully.")

preds, attn = model.predict(X_val)

import pandas as pd

pd.DataFrame({
    "actual": y_val.reshape(-1),
    "predicted": preds.reshape(-1)
}).to_csv("outputs/predictions/val_predictions.csv", index=False)

np.save("outputs/predictions/attention_weights.npy", attn)

print("Predictions & attention weights saved.")

import matplotlib.pyplot as plt

# Loss curve
plt.figure(figsize=(8,5))
plt.plot(history.history["loss"], label="loss")
plt.plot(history.history["val_loss"], label="val_loss")
plt.legend()
plt.title("Training Loss Curve")
plt.savefig("outputs/plots/training_loss_curve.png")
plt.close()

# Forecast plot
plt.figure(figsize=(10,5))
plt.plot(y_val[:200], label="Actual")
plt.plot(preds[:200], label="Prediction")
plt.legend()
plt.title("Predictions vs Actual")
plt.savefig("outputs/plots/forecast_plot.png")
plt.close()

print("Plots saved.")

import matplotlib.pyplot as plt
import numpy as np

sample_id = 0
att = attn[sample_id].reshape(-1)

plt.figure(figsize=(10,3))
plt.stem(att)
plt.title("Attention Weights for Sample 0")
plt.savefig("outputs/plots/attention_sample0.png")
plt.close()

print("Attention plot saved.")



import pandas as pd
import matplotlib.pyplot as plt

# Load the saved predictions
predictions_df = pd.read_csv("outputs/predictions/val_predictions.csv")

# Visualize actual vs. predicted values
plt.figure(figsize=(12, 6))
plt.plot(predictions_df["actual"], label="Actual")
plt.plot(predictions_df["predicted"], label="Predicted")
plt.title("Actual vs. Predicted Values")
plt.xlabel("Sample Index")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.savefig("outputs/plots/val_predictions_visualization.png")
plt.close()

print("Visualization of actual vs. predicted values saved to outputs/plots/val_predictions_visualization.png")

print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"R-squared (R2 Score): {r2:.4f}")

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Ensure preds and y_val are in the correct shape (1D arrays)
actual_values = y_val.reshape(-1)
predicted_values = preds.reshape(-1)

mse = mean_squared_error(actual_values, predicted_values)
mae = mean_absolute_error(actual_values, predicted_values)
r2 = r2_score(actual_values, predicted_values)

print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"R-squared (R2 Score): {r2:.4f}")

correlation = predictions_df['actual'].corr(predictions_df['predicted'])
print(f"Correlation between Actual and Predicted values: {correlation:.4f}")

from IPython.display import Image, display

image_path = "outputs/plots/val_predictions_visualization.png"
display(Image(filename=image_path))

